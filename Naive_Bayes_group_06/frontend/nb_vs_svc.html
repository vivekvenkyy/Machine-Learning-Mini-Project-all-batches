<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NB vs. SVC Comparison</title>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="app-container">
        <header class="app-header">
            <div class="logo">
                <h1>ML Dashboard</h1>
            </div>
            <nav>
                <a href="index.html" class="nav-link active">Back to Main Dashboard</a>
            </nav>
        </header>

        <main class="app-main">
            <section id="comparison-page" class="page">
                <div class="page-header">
                    <h2>Comparison: Naive Bayes vs. SVC</h2>
                </div>

                <div class="comparison-grid">
                    <div class="card">
                        <h3>Theoretical Knowledge: Naive Bayes</h3>
                        <p><strong>Core Idea:</strong> Based on Bayes' Theorem. It's called "naive" because it makes a strong assumption that all features are independent of each other, given the class.</p>
                        <p><strong>How it works:</strong> It calculates the probability of each class based on the probabilities of the features present in the data.</p>
                        <ul>
                            <li>✅ <strong>Pros:</strong> Very fast to train, works well with high-dimensional data (like text), and performs surprisingly well even when its independence assumption is violated.</li>
                            <li>❌ <strong>Cons:</strong> The independence assumption is almost never true in the real world. It can be outperformed by more complex models.</li>
                        </ul>
                    </div>
                    <div class="card">
                        <h3>Theoretical Knowledge: Support Vector Classifier (SVC)</h3>
                        <p><strong>Core Idea:</strong> Finds the "best" hyperplane (a line, plane, or higher-dimensional boundary) that separates the data into classes.</p>
                        <p><strong>How it works:</strong> It plots data points in N-dimensional space (where N is the number of features) and finds the boundary that has the maximum margin (distance) between the classes.</p>
                        <ul>
                            <li>✅ <strong>Pros:</strong> Very effective in high-dimensional spaces. Memory efficient (uses "support vectors"). Can model complex, non-linear relationships using different "kernels".</li>
                            <li>❌ <strong>Cons:</strong> Can be much slower to train, especially on large datasets. Less interpretable than models like Decision Trees.</li>
                        </ul>
                    </div>
                </div>

                <div id="results-container" class="hidden">
                    <div class="page-header" style="margin-top: 2rem;">
                        <h2>Live Analysis Results</h2>
                    </div>
                    </div>

                <div id="error-message" class="hidden">
                    Please <a href="index.html">run an analysis on the main dashboard</a> first to see the results here.
                </div>

            </section>
        </main>
    </div>
    <script src="comparison.js"></script>
</body>
</html>